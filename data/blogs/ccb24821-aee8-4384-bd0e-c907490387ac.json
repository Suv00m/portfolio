{
  "id": "ccb24821-aee8-4384-bd0e-c907490387ac",
  "title": "about my first llm which is called yuj",
  "description": "<p>earlier when im 20 year old, i have build a model called <strong>yuj</strong> which is a mixture of expert model that combines multiple specialized neural networks to tackle complex language tasks. it was specifically designed to work hindi which is our general language in India. The model showed promising results in handling Hindi text processing and translation tasks. </p><p></p><pre class=\"bg-gray-100 rounded-lg p-4 font-mono text-sm my-4\"><code>here is my configuration file :\nmodels:\n  - model: sarvamai/OpenHathi-7B-Hi-v0.1-Base\n    # no parameters necessary for base model\n  - model: ai4bharat/Airavata\n    parameters:\n      density: 0.5\n      weight: 0.5\n  - model: BhabhaAI/Gajendra-v0.1\n    parameters:\n      density: 0.5\n      weight: 0.3\nmerge_method: ties\nbase_model: sarvamai/OpenHathi-7B-Hi-v0.1-Base\nparameters:\n  normalize: true\ndtype: float16</code></pre><p>it is an 7b parameter and it is based ties method, and ties method is merging technique that combines multiple model weights while preserving the base model's structure and knowledge. </p><p></p><p>The ties method allows for effective knowledge integration from different models while maintaining computational efficiency and model stability. <br><br>and here is the code if you want to run on your own :<br></p><pre class=\"bg-gray-100 rounded-lg p-4 font-mono text-sm my-4\"><code># Usage\nimport torch\n\n# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# load the model in 4-bit quantization\ntokenizer = AutoTokenizer.from_pretrained(\"shuvom/yuj-v1\")\nmodel = AutoModelForCausalLM.from_pretrained(\"shuvom/yuj-v1\",torch_dtype=torch.bfloat16,load_in_4bit=True)\n\nprompt = \"युज शीर्ष द्विभाषी मॉडल में से एक है\"\ninputs = tokenizer(prompt, return_tensors=\"pt\")\n\n# Generate\ngenerate_ids = model.generate(inputs.input_ids, max_length=65)\ntokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]</code></pre><p>also there is the gguf format of this which helps you run it on local, and that is nice i feel:</p><p></p><p>first, you install this things:</p><pre class=\"bg-gray-100 rounded-lg p-4 font-mono text-sm my-4\"><code>!pip install llama-cpp-python huggingface-hub</code></pre><p></p><p>then you have to download the model:</p><pre class=\"bg-gray-100 rounded-lg p-4 font-mono text-sm my-4\"><code>!huggingface-cli download shuvom/yuj-v1-GGUF yuj-v1.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False</code></pre><p>then use this code:</p><pre class=\"bg-gray-100 rounded-lg p-4 font-mono text-sm my-4\"><code>from llama_cpp import Llama\n\nllm = Llama(\n  model_path=\"./yuj-v1.Q4_K_M.gguf\",  # Download the model file first\n  n_ctx=2048,  # The max sequence length to use - note that longer sequence lengths require much more resources\n  n_threads=8,            # The number of CPU threads to use, tailor to your system and the resulting performance\n  n_gpu_layers=35         # The number of layers to offload to GPU, if you have GPU acceleration available\n)</code></pre><p></p><p>Now here is brief about ties method:<br></p><p>The `ties` method determines how the model handles token embeddings between input and output layers, with options for weight sharing to reduce memory usage and potentially improve performance. <br><br>so basically it takes a sequence or sentence let's say, and convert into tokens and then use those token to route them to is actual expert, routing happens like this:<br></p><p>it (a small gated network for routing which calc softmax) calculate  and find out top k expert for that token and then those selected expert response in expert layer and then aggregation happens in which the we find out weighted sum and then it goes back to transformer flow, and that's how user get the response from a moe llm with <strong>ties</strong> method  </p>",
  "links": [],
  "created_at": "2026-01-14T18:17:04.277Z",
  "updated_at": "2026-01-15T04:57:27.668Z",
  "thumbnail": "https://images.pexels.com/photos/459335/pexels-photo-459335.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=650&w=940"
}